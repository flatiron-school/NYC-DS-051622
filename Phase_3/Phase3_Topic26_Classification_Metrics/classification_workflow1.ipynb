{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div style=\"color:white;\n",
    "           display:fill;\n",
    "           border-radius:5px;\n",
    "           background-color:#5642C5;\n",
    "           font-size:200%;\n",
    "           font-family:Arial;letter-spacing:0.5px\">\n",
    "\n",
    "<p width = 20%, style=\"padding: 10px;\n",
    "              color:white;\">\n",
    "Classification Workflow 1\n",
    "              \n",
    "</p>\n",
    "</div>\n",
    "\n",
    "Data Science Cohort Live NYC June 2022\n",
    "<p>Phase 3: Topic 26:</p>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<div align = \"right\">\n",
    "<img src=\"Images/flatiron-school-logo.png\" align = \"right\" width=\"200\"/>\n",
    "</div>\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# plot_confusion_matrix is a handy visual tool, added in the latest version of scikit-learn\n",
    "# if you are running an older version, comment out this line and just use confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import plot_roc_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> The goal here is to illustrate a possible workflow for classification modeling with `sklearn`'s `LogisticRegression` model.\n",
    "\n",
    "\n",
    "- Formulate and implement an iterative modeling workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Modeling Walkthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Caution! This notebook is very long and we will likely not get through it all. But the good news is that the modeling process is *iterative* and so after a few of those iterations you should get the hang of it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Build a model based on the [Titanic dataset](https://www.kaggle.com/c/titanic/data) that predicts whether a given person survived or not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "heading_collapsed": true,
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Modeling Steps\n",
    "## Data Preparation\n",
    "1. Explore data\n",
    "2. Feature Selection\n",
    "3. Separate out values for imputation\n",
    "4. Impute missing values\n",
    "5. One-hot encode categorical data\n",
    "6. Scale values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Fine Tuning\n",
    "7. Build a model\n",
    "8. Evaluate performance\n",
    "9. Make changes in an attempt to improve the model\n",
    "10. Demonstrate improvement was made"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This dataset has the following columns:\n",
    "\n",
    "| Variable | Definition | Key |\n",
    "| -------- | ---------- | --- |\n",
    "| survival | Survival | 0 = No, 1 = Yes |\n",
    "| pclass | Ticket class | 1 = 1st, 2 = 2nd, 3 = 3rd |\n",
    "| sex | Sex | |\n",
    "| Age | Age in years | |\n",
    "| sibsp | # of siblings / spouses aboard the Titanic | |\n",
    "| parch | # of parents / children aboard the Titanic | |\n",
    "| ticket | Ticket number | |\n",
    "| fare | Passenger fare | |\n",
    "| cabin | Cabin number | |\n",
    "| embarked | Port of Embarkation | C = Cherbourg, Q = Queenstown, S = Southampton |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Initial Data Understanding and Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Open up the file, get everything into `X` features and `y` target variables, divided into train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/titanic.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Which columns do you think we can let go of?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "What do you notice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "<details>\n",
    "    <summary><b><u>Observations</u></b></summary>\n",
    "\n",
    "38% Survived\n",
    "\n",
    "Missing vlaues in Age\n",
    "\n",
    "Its seems like some didn't pay\n",
    "\n",
    "Outliers in Fair/Sibsp look at mean and max\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 1st Model - Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's use a logistic regression and compare its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We're going to specifically avoid any regularization (the default) to see how the model does with little change. So we'll pass `'none'` to the `penalty` parameter to not use any regularization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> Note: you can and should add features incrementally in a \"real\" modeling context.  The engineering effort of encoding the variables can be non-trivial!  But here let's assume that it's not too much work to encode all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/titanic.csv\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#check data types \n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Check for missing target values\n",
    "df.Survived.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Feature Selection\n",
    "X = df.drop(['PassengerId','Name','Ticket','Embarked', 'Survived'],axis=1)\n",
    "y = df[\"Survived\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Handling Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. Drop missing values\n",
    "2. Imputation\n",
    "3. Impute and catalog where you imputed missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(X_train.isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "a = (X_train['Age'].isna().sum()/X_train.shape[0]).round(2) * 100\n",
    "c = (X_train['Cabin'].isna().sum()/X_train.shape[0]).round(2) * 100\n",
    "print(f'{a}% of Age is missing.')\n",
    "print(f'{c}% of Cabin is missing.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now that we've specified which values were originally missing, let's fill in those missing values.  This takes two separate imputers because we want to use the mean for numeric data and the majority class for categorical data.\n",
    "\n",
    "The `SimpleImputer` class fills in the mean value by default, so we'll have to override that for the categorical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X_train_numeric = X_train.select_dtypes(exclude=['object'])\n",
    "X_train_numeric.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X_train_categorical = X_train.select_dtypes(include=['object'])\n",
    "X_train_categorical.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# uses mean\n",
    "numeric_imputer = SimpleImputer()\n",
    "numeric_imputer.fit(X_train_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "categorical_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "categorical_imputer.fit(X_train_categorical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We'll build a function here to minimize our work of imputation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def impute_missing_values(X, imputer):\n",
    "    \"\"\"\n",
    "    Given a DataFrame and an imputer, use the imputer to fill in all\n",
    "    missing values in the DataFrame\n",
    "    \"\"\"\n",
    "    imputed_array = imputer.transform(X)\n",
    "    imputed_df = pd.DataFrame(imputed_array, columns=X.columns, index=X.index)\n",
    "    return imputed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X_train_numeric = impute_missing_values(X_train_numeric, numeric_imputer)\n",
    "X_train_categorical = impute_missing_values(X_train_categorical, categorical_imputer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Double-check to make sure that all of the missing values are gone:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X_train_imputed = pd.concat([X_train_numeric, X_train_categorical], axis=1)\n",
    "X_train_imputed.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X_train_imputed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## One-Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now that there are no missing values, convert all of the categorical features into numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def encode_and_concat_feature_train(X_train, feature_name):\n",
    "    \"\"\"\n",
    "    Helper function for transforming training data.  It takes in the full X dataframe and\n",
    "    feature name, makes a one-hot encoder, and returns the encoder as well as the dataframe\n",
    "    with that feature transformed into multiple columns of 1s and 0s\n",
    "    \"\"\"\n",
    "    # make a one-hot encoder and fit it to the training data\n",
    "    ohe = OneHotEncoder(categories=\"auto\", handle_unknown=\"ignore\")\n",
    "    single_feature_df = X_train[[feature_name]]\n",
    "    ohe.fit(single_feature_df)\n",
    "    \n",
    "    # call helper function that actually encodes the feature and concats it\n",
    "    X_train = encode_and_concat_feature(X_train, feature_name, ohe)\n",
    "    \n",
    "    return ohe, X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def encode_and_concat_feature(X, feature_name, ohe):\n",
    "    \"\"\"\n",
    "    Helper function for transforming a feature into multiple columns of 1s and 0s. Used\n",
    "    in both training and testing steps.  Takes in the full X dataframe, feature name, \n",
    "    and encoder, and returns the dataframe with that feature transformed into multiple\n",
    "    columns of 1s and 0s\n",
    "    \"\"\"\n",
    "    # create new one-hot encoded df based on the feature\n",
    "    single_feature_df = X[[feature_name]]\n",
    "    feature_array = ohe.transform(single_feature_df).toarray()\n",
    "    ohe_df = pd.DataFrame(feature_array, columns=ohe.categories_[0], index=X.index)\n",
    "    \n",
    "    # drop the old feature from X and concat the new one-hot encoded df\n",
    "    X = X.drop(feature_name, axis=1)\n",
    "    X = pd.concat([X, ohe_df], axis=1)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "categorical_feature_names = [\"Sex\", 'Cabin']\n",
    "\n",
    "\n",
    "\n",
    "encoders = {}\n",
    "\n",
    "for categorical_feature in categorical_feature_names:\n",
    "    ohe,X_train_imputed = encode_and_concat_feature_train(X_train_imputed, categorical_feature)\n",
    "    encoders[categorical_feature] = ohe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X_train_imputed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 1st Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's run a logistic regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "logreg_model = LogisticRegression(random_state=2022, penalty = 'none')\n",
    "logreg_model.fit(X_train_imputed, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "What happened there? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Answer: it wasn't able to find the minimum with this number of steps in gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hyperparameter Adjustments to the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's try a couple of stopgap measures to get the model to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### More Iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Allows for more iterations to find a solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "logreg_model_more_iterations = LogisticRegression(\n",
    "                                                random_state=2022, \n",
    "                                                penalty='none', \n",
    "                                                max_iter=5000\n",
    ")\n",
    "logreg_model_more_iterations.fit(X_train_imputed, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### More Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Remember that the `C` parameter is the inverse of the regularization strength."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> Note: We could do regularization but we should first scale our features. We're actually going to skip this hyperparameter until we scale our data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Higher Tolerance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A higher tolerance means that the model will stop training earlier (when predictions and true values aren't as close as they could be)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "logreg_model_higher_tolerance = LogisticRegression(\n",
    "                                                random_state=2022, \n",
    "                                                penalty='none', \n",
    "                                                tol=25\n",
    ")\n",
    "logreg_model_higher_tolerance.fit(X_train_imputed, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "fix, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 6))\n",
    "\n",
    "axes[0].set_title(\"More Iterations\")\n",
    "axes[1].set_title(\"Higher Tolerance\")\n",
    "\n",
    "plot_confusion_matrix(logreg_model_more_iterations, X_train_imputed, y_train,\n",
    "                      ax=axes[0], cmap=\"plasma\")\n",
    "plot_confusion_matrix(logreg_model_higher_tolerance, X_train_imputed, y_train,\n",
    "                      ax=axes[1], cmap=\"plasma\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "class ModelWithCV():\n",
    "    '''Structure to save the model and more easily see its crossvalidation'''\n",
    "    \n",
    "    def __init__(self, model, model_name, X, y, cv_now=True):\n",
    "        self.model = model\n",
    "        self.name = model_name\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        # For CV results\n",
    "        self.cv_results = None\n",
    "        self.cv_mean = None\n",
    "        self.cv_median = None\n",
    "        self.cv_std = None\n",
    "        #\n",
    "        if cv_now:\n",
    "            self.cross_validate()\n",
    "        \n",
    "    def cross_validate(self, X=None, y=None, kfolds=10):\n",
    "        '''\n",
    "        Perform cross-validation and return results.\n",
    "        \n",
    "        Args: \n",
    "          X:\n",
    "            Optional; Training data to perform CV on. Otherwise use X from object\n",
    "          y:\n",
    "            Optional; Training data to perform CV on. Otherwise use y from object\n",
    "          kfolds:\n",
    "            Optional; Number of folds for CV (default is 10)  \n",
    "        '''\n",
    "        \n",
    "        cv_X = X if X else self.X\n",
    "        cv_y = y if y else self.y\n",
    "\n",
    "        self.cv_results = cross_val_score(self.model, cv_X, cv_y, cv=kfolds)\n",
    "        self.cv_mean = np.mean(self.cv_results)\n",
    "        self.cv_median = np.median(self.cv_results)\n",
    "        self.cv_std = np.std(self.cv_results)\n",
    "\n",
    "        \n",
    "    def print_cv_summary(self):\n",
    "        cv_summary = (\n",
    "        f'''CV Results for `{self.name}` model:\n",
    "            {self.cv_mean:.5f} Â± {self.cv_std:.5f} accuracy\n",
    "        ''')\n",
    "        print(cv_summary)\n",
    "\n",
    "        \n",
    "    def plot_cv(self, ax):\n",
    "        '''\n",
    "        Plot the cross-validation values using the array of results and given \n",
    "        Axis for plotting.\n",
    "        '''\n",
    "        ax.set_title(f'CV Results for `{self.name}` Model')\n",
    "        # Thinner violinplot with higher bw\n",
    "        sns.violinplot(y=self.cv_results, ax=ax, bw=.4)\n",
    "        sns.swarmplot(\n",
    "                y=self.cv_results,\n",
    "                color='orange',\n",
    "                size=10,\n",
    "                alpha= 0.8,\n",
    "                ax=ax\n",
    "        )\n",
    "\n",
    "        return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "logreg_model_more_iterations_results = ModelWithCV(\n",
    "                                        logreg_model_more_iterations,\n",
    "                                        'more_iterations',\n",
    "                                        X_train_imputed,\n",
    "                                        y_train\n",
    ")\n",
    "    \n",
    "logreg_model_higher_tolerance_results = ModelWithCV(\n",
    "                                        logreg_model_higher_tolerance,\n",
    "                                        'higher_tolerance',\n",
    "                                        X_train_imputed,\n",
    "                                        y_train\n",
    ")\n",
    "\n",
    "model_results = [\n",
    "    logreg_model_more_iterations_results,\n",
    "    logreg_model_higher_tolerance_results\n",
    "]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "f,axes = plt.subplots(ncols=2, sharey=True, figsize=(12, 6))\n",
    "\n",
    "for ax, result in zip(axes, model_results):\n",
    "    ax = result.plot_cv(ax)\n",
    "    result.print_cv_summary()\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "plot_roc_curve(logreg_model_more_iterations, X_train_imputed, y_train, \n",
    "               name='logreg_model_more_iterations', ax=ax)\n",
    "plot_roc_curve(logreg_model_higher_tolerance, X_train_imputed, y_train, \n",
    "               name='logreg_model_higher_tolerance', ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Even More Data Preparation - Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "However, recall we should scale all of the features, so the model isn't overly penalizing age and fare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#We need to perform Feature Scaling when we are dealing with Gradient Descent Based algorithms (Linear and Logistic Regression, Neural Network) \n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(X_train_imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def scale_values(X, scaler):\n",
    "    \"\"\"\n",
    "    Given a DataFrame and a fitted scaler, use the scaler to scale all of the features\n",
    "    \"\"\"\n",
    "    scaled_array = scaler.transform(X)\n",
    "    scaled_df = pd.DataFrame(scaled_array, columns=X.columns, index=X.index)\n",
    "    return scaled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X_train_scaled = scale_values(X_train_imputed, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X_train_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 3rd Model - After Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now that the data is scaled, let's see if we can fit the model without tweaking any hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "logreg_model = LogisticRegression(random_state=2022)\n",
    "logreg_model.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model Evaluation, Part 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now that we are able to run a logistic regression with default hyperparameters, let's see how that performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "fig.suptitle(\"Logistic Regression with Features Scaled\")\n",
    "\n",
    "plot_confusion_matrix(logreg_model, X_train_scaled, y_train, ax=ax, cmap=\"plasma\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "scaled_features_results = ModelWithCV(\n",
    "                            logreg_model,\n",
    "                            'scaled_features',\n",
    "                            X_train_scaled,\n",
    "                            y_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Saving variable for convenience\n",
    "model_results = scaled_features_results\n",
    "\n",
    "# Plot CV results\n",
    "fig, ax = plt.subplots()\n",
    "ax = model_results.plot_cv(ax)\n",
    "plt.tight_layout();\n",
    "# Print CV results\n",
    "model_results.print_cv_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plot_roc_curve(logreg_model, X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "... this might be overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hyperparameter Adjustment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Different Regularization Strengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's try out some different regularization penalties to see if we can improve the test data score a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "model_results = [scaled_features_results]\n",
    "C_values = [0.0001, 0.001, 0.01, 0.1, 1]\n",
    "\n",
    "for c in C_values:\n",
    "    logreg_model = LogisticRegression(random_state=2022, C=c) #solver = \"liblinear\"\n",
    "    logreg_model.fit(X_train_scaled, y_train)\n",
    "    # Save Results\n",
    "    new_model_results = ModelWithCV(\n",
    "                            logreg_model,\n",
    "                            f'scaled_features_c{c:e}',\n",
    "                            X_train_scaled,\n",
    "                            y_train\n",
    "    )\n",
    "    model_results.append(new_model_results)\n",
    "    new_model_results.print_cv_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "f,axes = plt.subplots(ncols=3, nrows=2, sharey='all', figsize=(18, 12))\n",
    "\n",
    "for ax,result in zip(axes.ravel(),model_results):\n",
    "    ax = result.plot_cv(ax)\n",
    "\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "It looks like C = .01 value is pretty optimal for this solver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model_results = [scaled_features_results]\n",
    "scaled_features_cross_val_score = scaled_features_results.cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Different Solvers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "logreg_model = LogisticRegression(random_state=2022, C=0.01)\n",
    "logreg_model.fit(X_train_scaled, y_train)\n",
    "ModelWithCV(logreg_model, \"l2\", X_train_scaled, y_train).print_cv_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "logreg_model = LogisticRegression(random_state=2022, solver=\"liblinear\", C=.01)\n",
    "logreg_model.fit(X_train_scaled, y_train)\n",
    "ModelWithCV(logreg_model, \"liblinear\", X_train_scaled, y_train).print_cv_summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Save for later comparison\n",
    "model_results.append(\n",
    "    ModelWithCV(\n",
    "        logreg_model, \n",
    "        'C:0.01',\n",
    "        X_train_scaled,\n",
    "        y_train\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Save for later comparison\n",
    "model_results.append(\n",
    "    ModelWithCV(\n",
    "        logreg_model, \n",
    "        'solver:liblinear_C:0.01',\n",
    "        X_train_scaled,\n",
    "        y_train\n",
    "    )\n",
    ")\n",
    "\n",
    "# Plot both  models\n",
    "f,axes = plt.subplots(ncols=2, sharey='all', figsize=(12, 6))\n",
    "\n",
    "model_results[1].plot_cv(ax=axes[0])\n",
    "model_results[-1].plot_cv(ax=axes[1])\n",
    "\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "logreg_model = LogisticRegression(random_state=2022, C =.01)\n",
    "logreg_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "fig.suptitle(\"Logistic Regression with Features Scaled, Hyperparameters Tuned)\")\n",
    "\n",
    "plot_confusion_matrix(logreg_model, X_train_scaled, y_train, ax=ax, cmap=\"plasma\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## `SelectFromModel`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The last model is probably overfitting. We might try thinning out the number of features by eliminating the ones with small modeling coefficients using [`SelectFromModel`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "selector = SelectFromModel(logreg_model)\n",
    "\n",
    "selector.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We're using the default threshold here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "thresh = selector.threshold_\n",
    "thresh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's get a sense of which features will be eliminated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "coefs = selector.estimator_.coef_\n",
    "coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "coefs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "X_train_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "selector.get_support()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sup = selector.get_support()\n",
    "unique, counts = np.unique(sup, return_counts=True)\n",
    "print(np.asarray((unique, counts)).T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "dict(zip(X_train_scaled.columns, selector.get_support()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def select_important_features(X, selector):\n",
    "    \"\"\"\n",
    "    Given a DataFrame and a selector, use the selector to choose\n",
    "    the most important columns\n",
    "    \"\"\"\n",
    "    imps = dict(zip(X.columns, selector.get_support()))\n",
    "    selected_array = selector.transform(X)\n",
    "    selected_df = pd.DataFrame(selected_array,\n",
    "                               columns=[col for col in X.columns if imps[col]],\n",
    "                               index=X.index)\n",
    "    return selected_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X_train_selected = select_important_features(X=X_train_scaled, selector=selector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X_train_selected.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "logreg_sel = LogisticRegression(random_state=2022,C =.01)\n",
    "\n",
    "logreg_sel.fit(X_train_selected, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Probably still overfitting, but let's call this our final model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Final Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now that we have a final model, run X_test through all of the preprocessing steps so we can evaluate the model's performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X_test_no_transformations = X_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "numeric_feature_names = ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare']\n",
    "\n",
    "# separate out values for imputation\n",
    "X_test_numeric = X_test_no_transformations[numeric_feature_names]\n",
    "X_test_categorical = X_test_no_transformations[categorical_feature_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# impute missing values\n",
    "X_test_numeric = impute_missing_values(X_test_numeric, numeric_imputer)\n",
    "X_test_categorical = impute_missing_values(X_test_categorical, categorical_imputer)\n",
    "X_test_final = pd.concat([X_test_numeric, X_test_categorical], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# one-hot encode categorical data\n",
    "for categorical_feature in categorical_feature_names:\n",
    "    X_test_final = encode_and_concat_feature(X_test_final,\n",
    "                                       categorical_feature, encoders[categorical_feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# scale values\n",
    "X_test_scaled = scale_values(X_test_final, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# select features\n",
    "X_test_selected = select_important_features(X_test_scaled, selector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X_test_selected.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Create a model with the relevant hyperparameters, fit, and score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "final_model = LogisticRegression(random_state=2022, C =.01)\n",
    "final_model.fit(X_train_selected, y_train)\n",
    "\n",
    "final_model.score(X_test_selected, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Compare the past models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Create a way to categorize our different models\n",
    "model_candidates = [\n",
    "  \n",
    "    {\n",
    "        'name':'logreg_model_more_iterations'\n",
    "        ,'model':logreg_model_more_iterations\n",
    "        ,'X_test':X_test_final\n",
    "        ,'y_test':y_test\n",
    "    },\n",
    "    {\n",
    "        'name':'logreg_model_higher_tolerance'\n",
    "        ,'model':logreg_model_higher_tolerance\n",
    "        ,'X_test':X_test_final\n",
    "        ,'y_test':y_test\n",
    "    },\n",
    "    {\n",
    "        'name':'final_model'\n",
    "        ,'model':final_model\n",
    "        ,'X_test':X_test_selected\n",
    "        ,'y_test':y_test\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "final_scores_dict = {\n",
    "    \"Model Name\": [candidate.get('name') for candidate in model_candidates],\n",
    "    \"Mean Accuracy\": [\n",
    "        candidate.get('model').score(\n",
    "                                candidate.get('X_test'), \n",
    "                                candidate.get('y_test')\n",
    "        ) \n",
    "        for candidate in model_candidates\n",
    "    ]\n",
    "    \n",
    "}\n",
    "final_scores_df = pd.DataFrame(final_scores_dict).set_index('Model Name')\n",
    "final_scores_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Final comparison of confusion matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "nrows = 2\n",
    "ncols = math.ceil(len(model_candidates)/nrows)\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "                nrows=nrows,\n",
    "                ncols=ncols,\n",
    "                figsize=(12, 6)\n",
    ")\n",
    "fig.suptitle(\"Confusion Matrix Comparison\")\n",
    "\n",
    "# Turn off all the axes (in case nothing to plot); turn on while iterating over\n",
    "[ax.axis('off') for ax in axes.ravel()]\n",
    "\n",
    "\n",
    "for i,candidate in enumerate(model_candidates):\n",
    "    # Logic for making rows and columns for matrices\n",
    "    row = i // 2\n",
    "    col = i % 2\n",
    "    ax = axes[row][col]\n",
    "    \n",
    "    ax.set_title(candidate.get('name'))\n",
    "    ax.set_axis_on() \n",
    "    cm_display = plot_confusion_matrix(\n",
    "                    candidate.get('model'),\n",
    "                    candidate.get('X_test'),\n",
    "                    candidate.get('y_test'),\n",
    "                    normalize='true',\n",
    "                    cmap='plasma',\n",
    "                    ax=ax,\n",
    "                    \n",
    "    )\n",
    "    cm_display.im_.set_clim(0, 1)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Plot only the last models we created (so it's not too cluttered)\n",
    "for model_candidate in model_candidates[0:]:\n",
    "    plot_roc_curve(\n",
    "        model_candidate.get('model'),\n",
    "        model_candidate.get('X_test'),\n",
    "        model_candidate.get('y_test'), \n",
    "        name=model_candidate.get('name'),\n",
    "        ax=ax\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "Where was there data leakage?\n",
    "\n",
    "What you add to data preparation process?\n",
    "\n",
    "How would select your features?\n",
    "\n",
    "How would deal with outliers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Build and iterate on a logistic regression model of **color** for the diamonds dataset! Maximize accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "diamonds = sns.load_dataset('diamonds')"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "TOC",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
