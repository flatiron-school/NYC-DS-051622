{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79adf0f5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div style=\"color:white;\n",
    "           display:fill;\n",
    "           border-radius:5px;\n",
    "           background-color:#5642C5;\n",
    "           font-size:200%;\n",
    "           font-\\amily:Arial;letter-spacing:0.5px\">\n",
    "\n",
    "<p width = 20%, style=\"padding: 10px;\n",
    "              color:white;\">\n",
    "Regularization\n",
    "              \n",
    "</p>\n",
    "</div>\n",
    "\n",
    "Data Science Cohort Live NYC June 2022\n",
    "<p>Phase 3: Topic 24</p>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<div align = \"right\">\n",
    "<img src=\"Images/flatiron-school-logo.png\" align = \"right\" width=\"200\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1dfee06",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Objectives\n",
    "- Explain the notion of \"validation data\"\n",
    "- Use the algorithm of cross-validation (with `sklearn`)\n",
    "- Explain the concept of regularization\n",
    "- Use Lasso and Ridge regularization in model design"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdba225",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The balance between underfitting vs. overfitting:\n",
    "- A central problem in machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f24baa",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Adding more and more features to get lower error on train set.\n",
    "- E.g. more polynomial columns via Polynomial transformer, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aede0be9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Model has more flexibility with more fitting parameters:\n",
    "- More freedom to adjust weights to lower cost function on training data: mean squared error, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f1d571",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "But freedom can be a curse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91b78dd",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src = \"Images/overfitting.png\" width = 300/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ceffbf2",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Flexibility for weights to be optimized to fit to train data exactly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae3cbfb",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Flexibile model can tune floppy weights unrealistically high (or low) to fit data.\n",
    "\n",
    "- Fits data with weight vector $\\textbf{w}$: \n",
    "    - many of the features may have significant coefficients \n",
    "    - large positive weights for some\n",
    "    - large negative weights for others\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0b7116",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Has learned a specific complex model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d5ebf6",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "But remember:\n",
    "- Training data is a **sample**.\n",
    "\n",
    "If we take another sample from population, is it likely to learn the same weight vector $\\textbf{w}$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c03c12b",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src = \"Images/overfitting.png\" width = 300/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d566ea0",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Likely to learn another $\\textbf{w}$ that is significantly different."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c449df6",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "I.e. trains and learns a different specific complex model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a0f37c",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Keep playing this game:\n",
    "- large spread in $\\textbf{w}$ on different training samples.\n",
    "- Components of $\\textbf{w}$ floppy. Can take on large and varied values. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93de6662",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Variance in \"bias-variance\" has a precise statistical meaning:\n",
    "\n",
    "$$ \\mathrm{Var}[\\textbf{w}] $$ where  the weight vector $ \\textbf{w} $ is a random variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56908f1b",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "An overfitted model with more features than necessary (i.e. an overflexible model):\n",
    "- Typically means high $\\mathrm{Var}[\\textbf{w}]$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c398b8e",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "High $\\mathrm{Var}[\\textbf{w}]$:\n",
    "- Significantly different and specific values of $\\textbf{w}$ and learned function on each training set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cbcd3e",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Then expect that:\n",
    "-  learned function doesn't represent the average tendency of target vs features very well.\n",
    "- higher error on unseen/test data.\n",
    "- poorer predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398bb014",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Multicollinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1194921",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We talked about this way back. But how does it increase Var[$\\textbf{w}$]?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1704ff6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da55d1a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "WHO_data = pd.read_csv(\"data/WHO_life.csv\")\n",
    "X_WHO = WHO_data.drop(columns = [\"Life expectancy \"])\n",
    "y = WHO_data[\"Life expectancy \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8dd4f6d",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "X_WHO.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16282345",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Many features from WHO dataset:\n",
    "\n",
    "Regressing to find weights life expectancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47166a1b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X_WHO.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4af08a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "But let's take a look at a few of these and their correlations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a98830",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "col_selector = ['Income composition of resources', 'Schooling','Alcohol', ' thinness  1-19 years']\n",
    "subsetX = X_WHO[col_selector]\n",
    "sns.heatmap(subsetX.corr())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b144992f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's focus on Schooling and income composite resources (ICR):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7276cad",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ Life= w_1*Alcohol + w_2*Polio + w_3*Schooling + w_4*Measles + w_5*ICR + ... $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e6f8e9",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Correlation is very high!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ba4dd4",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "col_selector = ['Income composition of resources', 'Schooling']\n",
    "X_WHO[col_selector].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2498e01d",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Our regression: \n",
    "- Y = life expectancy\n",
    "\n",
    "$$ Y - \\sum_{i \\neq 3,5} w_i x_i = w_3 Schooling + w_5 ICR $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c26724a",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Schooling and ICR highly related:\n",
    "\n",
    "- Implies that $w_3$ and $w_5$ introduce too much flexibility.\n",
    "- Maybe could fit almost as well with just $w_3$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89056b45",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- $w_3$ and $w_5$ are floppy and can become big in either direction to fit data.\n",
    "- Var[$\\textbf{w}$] from $w_3$ and $w_5$ high."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b09965e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### How to assess model variance: cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43237dcc",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Could get many different training sets:\n",
    "- Train weights $\\textbf{w}$ for each.\n",
    "- Get variance of $\\textbf{w}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb76059",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Semi-equivalently:\n",
    "- Test performance of each model on test set.\n",
    "- Evaluate model performance/variance by looking at average/standard deviation of performance on test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d56a10",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Problem: \n",
    "- likely don't have this much data available to make enough independent training sets large enough to for each model to train on effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a2b769",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Solution: Cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011b2821",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src = \"Images/crossval.png\"  width = 800/>\n",
    "<center> Splitting up training set </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee35c4d",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Split up training set into folds:\n",
    "- Training fold\n",
    "- Validation fold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67599a49",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- For each iteration:\n",
    "    - train a model.\n",
    "    - Test on validation fold. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00a0ea6",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Effectively sampling multiple training sets:\n",
    "- testing each model performance on different **validation set**.\n",
    "- Good for estimating model performance on average\n",
    "- Good for estimating model variance as well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd02b829",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "So in the end:\n",
    "- Performance metrics measured on validation\n",
    "- We get average performance metric across all the models for each cross validation iteration.\n",
    "- Get variance of performance metric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39672d6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Note: **validation set** is part of training set:\n",
    "- Not part of true test/hold-out set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87de86f7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Idea is that we tune / figure out how to limit variance using training set:\n",
    "- Since limited data:\n",
    "    - Use train set to train and estimate model performance and variance while tuning.\n",
    "- But model hase \"seen\" validation set:\n",
    "\n",
    "True and final evaluation:\n",
    "- Measure performance on tuned model on the test set that has never been seen before."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8a88af",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Roughly:\n",
    "- Training data is for building the model;\n",
    "- Validation data is for *tweaking* the model;\n",
    "- Testing data is for evaluating the model on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f699ee",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Think of **training** data as what you study for a test\n",
    "- Think of **validation** data is using a practice test (note sometimes called **dev**)\n",
    "- Think of **testing** data as what you use to judge the model\n",
    "    - A **holdout** set is when your test dataset is never used for training (unlike in cross-validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9fdbf1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](https://scikit-learn.org/stable/_images/grid_search_workflow.png)\n",
    "> Image from Scikit-Learn https://scikit-learn.org/stable/modules/cross_validation.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a5ed7e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "1. Split data into training data and a holdout test\n",
    "2. Design a model\n",
    "3. Evaluate how well it generalizes with **cross-validation** (only training data)\n",
    "4. Determine if we should adjust model, use cross-validation to evaluate, and repeat\n",
    "5. After iteratively adjusting your model, do a _final_ evaluation with the holdout test set\n",
    "6. DON'T TOUCH THE MODEL!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6452054",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Ways to limit/deal with high variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7b9508",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Get more data. With enough training data, even with floppy weights it'll get it right."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67aae30f",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Yeah, but often not possible/easy to get enough data for this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d846a64e",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Get rid of columns that exhibit a high degree of collinearity with other columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecf3b38",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Yeah, but did we throw out some useful information for prediction? \n",
    "- ICR and schooling not the same thing.\n",
    "- How many of the collinear columns should we throw away? Which ones?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a679fcd",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Or we could come up with ways to directly limit the variance through the cost function itself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662089c6",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's try this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58396d23",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Preventing Overfitting - Regularization\n",
    "Again, complex models are very flexible in the patterns that they can model but this also means that they can easily find patterns that are simply statistical flukes of one particular dataset rather than patterns reflective of the underlying data-generating process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2eb4ac3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "When a model has large weights, the model is \"too confident\". This translates to a model with high variance which puts it in danger of overfitting!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75efc955",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](images/punishing_model_metaphor.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb75f254",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We need to punish large (confident) weights by contributing them to the error function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8391e3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Some Types of Regularization:**\n",
    "\n",
    "1. Reducing the number of features\n",
    "2. Increasing the amount of data\n",
    "3. Popular techniques: Ridge, Lasso, Elastic Net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39232bd9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Regularization for Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05c5d0a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Modify our squared error loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96060411",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ L = |\\textbf{y} - X \\textbf{w}|^2 + \\lambda |\\textbf{w}|^2 $$\n",
    "\n",
    "with $|\\textbf{w}|^2 = w_1^2 + w_2^2 + ... + w_m^2$ as sum of squares of the feature weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdcd374",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src = \"Images/ridge_regression_geometric.png\" width = 450>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f5cdd1",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Tug of war between:\n",
    "\n",
    "Ridge cost: $ \\lambda |\\textbf{w}|^2 = \\lambda (w_1^2 + w_2^2)  $\n",
    "- L2 Regularization (Euclidean distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ec07e0",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Least squares cost: $ |\\textbf{y} - X\\textbf{w}|^2 $\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9fae68",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src = \"Images/ridge_regression_geometric.png\" width = 450>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839b2dae",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Penalizes large weights: \n",
    "- **strongly** discourages large fluctuations in $\\textbf{w}$ depending on training set.\n",
    "- i.e. reduces Var[$\\textbf{w}$]\n",
    "- **Can lead to large performance boost on unseen data.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89552a9f",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- the tradeoff: repeating fitting on large number of distinct training sets (samples of population:\n",
    "    - Average of $\\textbf{w}$ is a little off from best fit to population"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e32264a",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "An oft-used picture:\n",
    "\n",
    "<img src = \"Images/biasvar_bullseye.png\" width = 400/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc220f63",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This really refers to the weights!\n",
    "- Bullseye is $\\textbf{w}$ fit on entire (unknown/unseen) population.\n",
    "- Each dot is $\\textbf{w}$ trained on a different sample (training set)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a950a3",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "An oft-used picture:\n",
    "\n",
    "<img src = \"Images/biasvar_bullseye.png\" width = 400/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10570897",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Idea of Ridge: \n",
    "- Tune $\\lambda$ just right. This is something we input as external parameter to model. **Hyperparameter** \n",
    "- Cluster of $\\textbf{w}$ little off the center of the bullseye\n",
    "- But: tightly clustered."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95ee25e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "OK let's do a Ridge regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbca8ef1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# model validation: testing model variance with cross validation\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e47efe",
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "birds = sns.load_dataset('penguins')\n",
    "birds = birds.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df85aabe",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "birds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a134327",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Going to use the other features to predict the body mass of a penguin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e099fd4d",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X = birds.drop('body_mass_g', axis=1)\n",
    "y = birds['body_mass_g']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02e781c",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X ,y, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83127842",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's one-hot encode the nominal categoricals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd4e29a",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Taking in other features (category)\n",
    "ohe = OneHotEncoder(drop='first')\n",
    "dummies = ohe.fit_transform(X_train[['species', 'island', 'sex']])\n",
    "\n",
    "# Getting a DF\n",
    "X_train_onehot = pd.DataFrame(dummies.todense(), columns=ohe.get_feature_names(), index=X_train.index)\n",
    "\n",
    "X_train_onehot.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9072c0e6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Attach this to dataframe with numerical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f8ee37",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X_train_numeric = X_train[['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm']]\n",
    "X_train_df = pd.concat([X_train_numeric, X_train_onehot], axis=1)\n",
    "X_train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbd822a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We have our training feature matrix:\n",
    "- Apply transformation fit_transformed on train set to test feature matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79229562",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "test_dummies = ohe.transform(X_test[['species', 'island', 'sex']])\n",
    "test_df = pd.DataFrame(test_dummies.todense(), columns=ohe.get_feature_names(),\n",
    "                       index=X_test.index)\n",
    "X_test_df = pd.concat([X_test[['bill_length_mm', 'bill_depth_mm',\n",
    "                              'flipper_length_mm']], test_df], axis=1)\n",
    "X_test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a171ff",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "lr1 = LinearRegression()\n",
    "lr1.fit(X_train_df, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b82f9b",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "lr1.score(X_train_df, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bec397",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "y_pred = lr1.predict(X_test_df)\n",
    "np.sqrt(mean_squared_error(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03424149",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Wow that's a good $R^2$ value!\n",
    "\n",
    "- Estimate how we are doing on unseen data with cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b967516d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "cv_results = cross_validate(X=X_train_df, y=y_train, estimator=lr1, cv=10, scoring=('r2', 'neg_mean_squared_error'),\n",
    "                return_train_score=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983762c4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How well model explains training fold data in each iteration cross validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a12213a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "train_res = cv_results['train_r2']\n",
    "train_res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2d2788",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "It's fitting well each time in the 10 iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9613defb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How does the validation look?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e29199a",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "test_res = cv_results['test_r2']\n",
    "test_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17733a51",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "test_res.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612080c5",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "test_res.std(ddof = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a86b87c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Good average test performance and relatively low variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2038f5d",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "RMSE_train = np.sqrt(np.abs(cv_results['train_neg_mean_squared_error']))\n",
    "RMSE_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62a855a",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "RMSE_train.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2dcbee8",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "RMSE_train.std(ddof = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32d9d5c",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "RMSE_test = np.sqrt(np.abs(cv_results['test_neg_mean_squared_error']))\n",
    "RMSE_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695703cf",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Clearly larger average RMSE and variance of RMSE in the test set. But still pretty decent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbebd585",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "RMSE_test.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac50032",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "RMSE_test.std(ddof = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758cfd3e",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "But...I'm a greedy man.\n",
    "\n",
    "I want to do better than this. \n",
    "- I'm going to add some polynomials to get a more complex model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffb6ed8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "My motivation:\n",
    "    \n",
    "- More complex model = better able to capture more complex relationships between mass and other variables.\n",
    "- Better prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab31c497",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Adding model complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b555934",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c2b759",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pf = PolynomialFeatures(degree=3)\n",
    "X_poly_train = pd.DataFrame(pf.fit_transform(X_train_df))\n",
    "\n",
    "#y_train.to_csv('X_penguin.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2492b8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X_poly_test = pf.transform(X_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdce38e",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X_train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6ac70f",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X_poly_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e56ebaa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "poly_lr = LinearRegression()\n",
    "poly_lr.fit(X_poly_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4423ed",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "poly_lr.score(X_poly_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f53de1",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "lr1.score(X_train_df, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b99352e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "About a 3% improvement: \n",
    "- that could mean money in other contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6b2724",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "poly_cv_results = cross_validate(\n",
    "                X=X_poly_train, \n",
    "                y=y_train,\n",
    "                estimator=poly_lr, \n",
    "                cv=10,\n",
    "                scoring=('r2', 'neg_mean_squared_error'),\n",
    "                return_train_score=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e3720c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "poly_train_res = poly_cv_results['train_r2']\n",
    "poly_train_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde2e53a",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "poly_train_res.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e346fc76",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "poly_train_res.std(ddof =1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99963559",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Wow...I'm ready to brag to my boss.\n",
    "\n",
    "- But let's check the performance on the validation folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513a58fa",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "poly_valid_res = poly_cv_results['test_r2']\n",
    "poly_valid_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64beacb9",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "poly_valid_res.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4bb5c4",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "poly_valid_res.std(ddof = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265c2d92",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "RMSE_polytest = np.sqrt(np.abs(poly_cv_results['test_neg_mean_squared_error']))\n",
    "RMSE_polytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44768992",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "RMSE_polytest.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e35df06",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "RMSE_polytest.std(ddof=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2e7e28",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src = \"Images/punch_chuck_norris.gif\" width = 400/></center>\n",
    "<center>You just got punched in the face by the bias-variance problem.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b269524",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Fit an overly complex model:\n",
    "- Doesn't generalize well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82adb691",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's try regularizing polynomial model:\n",
    "$$  L = ||\\textbf{y} - X \\textbf{w}||_2^2 + \\lambda |\\textbf{w}|_2^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1c7cf3",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "- reduce the floppiness/complexity of model\n",
    "- but still keep *some* of the complexity added by these polynomial features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbec4ee4",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Reduce Var[$\\textbf{w}$].\n",
    "- Get model predictions more representative of population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186734db",
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "pf = PolynomialFeatures(degree=3)\n",
    "\n",
    "# You should always be sure to _standardize_ your data before\n",
    "# applying regularization!\n",
    "\n",
    "X_train_processed = pf.fit_transform(ss.fit_transform(X_train_df))\n",
    "X_test_processed = pf.transform(ss.transform(X_test_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2265328c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Absolutely need to standardize/normalize features:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a6111c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " $$ L = ||\\textbf{y} - X \\textbf{w}||_2^2 + \\lambda |\\textbf{w}|_2^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14509987",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "L2 regularization cost function makes no sense otherwise. \n",
    "- Weights will be on different scales if features not normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9240df57",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# 'Lambda' is the standard variable for the strength of the\n",
    "# regularization (as in the above formulas), but since lambda\n",
    "# is a key word in Python, these sklearn regularization tools\n",
    "# use 'alpha' instead.\n",
    "\n",
    "rr = Ridge(alpha=100, random_state=42)\n",
    "\n",
    "rr.fit(X_train_processed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4897904a",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "rr.score(X_train_processed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bc1c8a",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "lr1.score(X_train_df, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96142a7a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let' s cross validate this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcaf4315",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "rr_cv_results = cross_validate(\n",
    "                X=X_train_processed, \n",
    "                y=y_train,\n",
    "                estimator=rr, \n",
    "                cv=10,\n",
    "                scoring=('r2', 'neg_mean_squared_error'),\n",
    "                return_train_score=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3097e9d0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Get $R^2$ on train folds of cross validation trials:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f7d719",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "rr_cv_results['train_r2']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9891b0b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Get $R^2$ on validation folds of cross validation trials:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b16b428",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "rr_cv_results['test_r2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c018ea9b",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "rr_cv_results['test_r2'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0502ae6",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "rr_cv_results['test_r2'].std(ddof = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f668e8c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "RMSE_rrtest = np.sqrt(np.abs(rr_cv_results['test_neg_mean_squared_error']))\n",
    "RMSE_rrtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6145010",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "RMSE_rrtest.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4796d90",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "RMSE_rrtest.std(ddof = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77155102",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "L2 regularized polynomial model:\n",
    "- A little bit worse than my basic linear model.\n",
    "- Much much better than the un-regularized polynomial model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d059e368",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Could I do better? Possibly.\n",
    "\n",
    "- Tune hyperparameter $\\lambda$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0fe796",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Tuning hyperparameters\n",
    "\n",
    "- Don't know what $\\lambda$ will allow model to perform best on validation sets.\n",
    "- Need to tune this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310bb513",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Adjust model stiffness/regularization parameter\n",
    "- Assess model performance in validation testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ca4348",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Optimizing the Regularization Hyperparameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134e0dd0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### The most basic hyperparameter tuning method: Make a loop!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcbb9b4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The regularization strength could sensibly be any nonnegative number, so there's no way to check \"all possible\" values. It's often useful to try several values that are different orders of magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc79f54",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "alphas = [1e-3, 1e-2, 1e-1, 1, 10, 100, 1e3, 1e4]\n",
    "cv_scores = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    rr = Ridge(alpha=alpha, random_state=42)\n",
    "    cv_loop_results = cross_validate(\n",
    "                X=X_train_processed, \n",
    "                y=y_train,\n",
    "                estimator=rr, \n",
    "                cv=10,\n",
    "                scoring=('neg_mean_squared_error'))\n",
    "    cv_scores.append(np.mean(np.sqrt(np.abs(cv_loop_results['test_score']))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0ad821",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "sns.lineplot(x = np.log10(alphas), y = cv_scores, marker = 's', ax = ax)\n",
    "ax.set_xlabel('Log(lambda)')\n",
    "ax.set_ylabel('Mean RMSE')\n",
    "ax.set_title('RMSE averaged on validation folds')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c176c935",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Could fine tune:\n",
    "- But of hyperparameter values tried $\\lambda = 100$ is best."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24158bcb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now we *finally* report results on the true test set:\n",
    "- We have not fit optimized on it.\n",
    "- Have not tuned hyperparameters to see how well it performs on validation folds.\n",
    "\n",
    "**Test/hold-out set is our true final gold standard**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba9d2b6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "rr = Ridge(alpha = 100, random_state = 42)\n",
    "rr.fit(X_train_processed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ea6a91",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = rr.predict(X_test_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fac083",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "ridge_RMSE_holdout = np.sqrt(mean_squared_error(y_pred, y_test))\n",
    "ridge_RMSE_holdout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7444db",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Given the scale of the penguin body mass (g): this is good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95174519",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sns.histplot(y);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09604ad8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Takeaways of what we just did"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7030d2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Polynomial model: poor prediction performance.\n",
    "- L2 regularized the polynomial regression model (Ridge regression) + tuning\n",
    "- **Much** better test performance than unregularized polynomial model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36b7f5b",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "But:\n",
    "    \n",
    "- Our simple linear model with no polynomial worked well.\n",
    "- Almost as well as polynomial features\n",
    "- Think carefully before adding model complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb74a08",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A simple model with less number of good quality predictive features may work as well if not better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad4e40f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Logical extension:\n",
    "- Features that are correlated but don't want to throw them away.\n",
    "- L2 regularized linear model + tuning:\n",
    "    - don't throw out features.\n",
    "    - get better test performance than OLS by reducing weight variance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8d5e7d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Sometimes though: throwing away features might work better:\n",
    "- Learn good features only with high predictive power\n",
    "- Chuck the rest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f92a67c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### L1 Regularization (LASSO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a2b5f4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ L = ||\\textbf{y} - X \\textbf{w}||_2^2 + \\lambda ||\\textbf{w}||_1 $$\n",
    "\n",
    "with $||\\textbf{w}||_1 = |w_1| + |w_2| + ... + |w_m|$ as sum of absolute magnitude of the feature weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c66c03",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Taxi cab vs Euclidean distance:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d12cd8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src = \"Images/metrics.png\" width = 450/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555609e1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Circle in terms of L2 vs L1 distance:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bb0240",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\"Circle\" in L1:\n",
    "<img src = \"Images/taxcabgeometry.jpg\" width = 400/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cb0914",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\"Circles\" for different metrics\n",
    "\n",
    "<img src = \"Images/circles.png\" width = 400/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c03c0a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Why use the L1 magnitude $||\\textbf{w}||_1$ for regularization?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd08c7c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "LASSO encourages model weight sparsity: \n",
    "- prefers to drive weights $w_i$ for features with little predictive power to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743c89e8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src = \"Images/different_metric_regularization.png\" width = 600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc13e5e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Perform LASSO regression with scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b05e734",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9719204",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Find the best LASSO model: tune regularization hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b9aa65",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "alphas = [1, 10, 100, 1e3, 1e4]\n",
    "cv_lasso_scores = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    lasso = Lasso(alpha=alpha, random_state=42, max_iter = 10000)\n",
    "    cv_loop_results = cross_validate(\n",
    "                X=X_train_processed, \n",
    "                y=y_train,\n",
    "                estimator=lasso, \n",
    "                cv=10,\n",
    "                scoring=('neg_mean_squared_error'))\n",
    "    cv_lasso_scores.append(np.mean(np.sqrt(np.abs(cv_loop_results['test_score']))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ac7411",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8dab90",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "cv_lasso_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e4d7a9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The best LASSO model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342e20bc",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "lasso_opt = Lasso(alpha=10, random_state=42,  max_iter = 100000)\n",
    "lasso_opt.fit(X_train_processed, y_train)\n",
    "\n",
    "y_pred = lasso_opt.predict(X_test_processed) # get final test prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f131c4a9",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "lasso_RMSE = np.sqrt(mean_squared_error(y_pred, y_test))\n",
    "lasso_RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8832e94a",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "ridge_RMSE_holdout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d961f1e9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Comparable between ridge and LASSO. LASSO tends to have higher weight variance than ridge.\n",
    "\n",
    "But what's the real difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b019d5",
   "metadata": {
    "cell_style": "center",
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(rr.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f96d93",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "lasso_opt.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286eca2d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Strategy Behind Ridge / Lasso / Elastic Net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbee2e1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Overfit models overestimate the relevance that predictors have for a target. Thus overfit models tend to have **overly large coefficients**. \n",
    "\n",
    "Generally, overfitting models come from a result of high model variance. High model variance can be caused by:\n",
    "\n",
    "- having irrelevant or too many predictors\n",
    "- multicollinearity\n",
    "- large coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16e39e3",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Ridge \n",
    "When we introduce many features that:\n",
    "- we believe may all have some predictive power.\n",
    "- want to heavily penalize weight variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbc2f51",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### LASSO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b47f3a",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We have dataset with many highly correlated features:\n",
    "- believe many are not actually adding to predictive power.\n",
    "- willing to cut away marginally unimportant features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab46955",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Which is better:\n",
    "- depends on dataset\n",
    "- modeling goal "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70fb6962",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LEVEL UP - Elastic Net!\n",
    "Naturally, the Elastic Net has the same interface through sklearn as the other regularization tools! The only difference is that we now have to specify how much of each regularization term we want. The name of the parameter for this (represented by $\\rho$ above) in sklearn is `l1_ratio`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ddf857",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "enet = ElasticNet(alpha=10, l1_ratio=0.1, random_state=42)\n",
    "\n",
    "enet.fit(X_train_processed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ee0ea1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "enet.score(X_train_processed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d94b4e7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "enet.score(X_test_processed, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2ce0cc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Setting the `l1_ratio` to 1 is equivalent to the lasso:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba80c713",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "ratios = np.linspace(0.01, 1, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a65c50",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "preds = []\n",
    "for ratio in ratios:\n",
    "    enet = ElasticNet(alpha=100, l1_ratio=ratio, random_state=42)\n",
    "    enet.fit(X_train_processed, y_train)\n",
    "    preds.append(enet.predict(X_test_processed[0].reshape(1, -1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9335e2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "lasso = Lasso(alpha=100, random_state=42)\n",
    "lasso.fit(X_train_processed, y_train)\n",
    "lasso_pred = lasso.predict(X_test_processed[0].reshape(1, -1))\n",
    "\n",
    "ax.plot(ratios, preds, label='elastic net')\n",
    "ax.scatter(1, lasso_pred, c='k', s=70, label='lasso')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab58565",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Note on `ElasticNet()`\n",
    "Is an Elastic Net with `l1_ratio` set to 0 equivalent to the ridge? In theory yes. But in practice no. It looks like the `ElasticNet()` predictions on the first test data point as `l1_ratio` shrinks are tending toward some value around 3400. Let's check to see what prediction `Ridge()` gives us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d527066",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "ridge = Ridge(alpha=10, random_state=42)\n",
    "ridge.fit(X_train_processed, y_train)\n",
    "ridge.predict(X_test_processed[0].reshape(1, -1))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2c1571",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "If you check the docstring for the `ElasticNet()` class you will see:\n",
    "- that the function being minimized is slightly different from what we saw above; and\n",
    "- that the results are unreliable when `l1_ratio` $\\leq 0.01$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517a3725",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Exercise**: Visualize the difference in this case between `ElasticNet(l1_ratio=0.01)` and `Ridge()` by making a scatterplot of each model's predicted values for the first ten points in `X_test_processed`. Use `alpha=10` for each model.\n",
    "\n",
    "        Level Up: Make a second scatterplot that compares the predictions on the same data\n",
    "        points between ElasticNet(l1_ratio=1) and Lasso()."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c7e2bd",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<details>\n",
    "    <summary> Answer\n",
    "    </summary>\n",
    "    <code>fig, ax = plt.subplots()\n",
    "enet_r = ElasticNet(alpha=10, l1_ratio=0.01, random_state=42)\n",
    "enet_r.fit(X_train_processed, y_train)\n",
    "preds_enr = enet_r.predict(X_test_processed[:10])\n",
    "preds_ridge = ridge.predict(X_test_processed[:10])\n",
    "ax.scatter(np.arange(10), preds_enr)\n",
    "ax.scatter(np.arange(10), preds_ridge);</code>  \n",
    "        </details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c2484d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<details>\n",
    "    <summary>\n",
    "        Level Up Answer\n",
    "    </summary>\n",
    "<code>fig, ax = plt.subplots()\n",
    "enet_l = ElasticNet(alpha=10, l1_ratio=1, random_state=42)\n",
    "enet_l.fit(X_train_processed, y_train)\n",
    "preds_enl = enet_l.predict(X_test_processed[:10])\n",
    "preds_lasso = lasso.predict(X_test_processed[:10])\n",
    "ax.scatter(np.arange(10), preds_enl)\n",
    "ax.scatter(np.arange(10), preds_lasso);</code>\n",
    "    </details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54323b69",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Fitting Regularized Models with Cross-Validation\n",
    "Our friend `sklearn` also includes tools that fit regularized regressions *with cross-validation*: `LassoCV`, `RidgeCV`, and `ElasticNetCV`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc49486",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Exercise**: Use `RidgeCV` to fit a seven-fold cross-validated ridge regression model to our `X_train_processed` data and then calculate $R^2$ and the RMSE (root-mean-squared error) on our test set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f245e82e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<details>\n",
    "    <summary>\n",
    "        Answer\n",
    "    </summary>\n",
    "    <code>rcv = RidgeCV(cv=7)\n",
    "rcv.fit(X_train_processed, y_train)\n",
    "rcv.score(X_test_processed, y_test)\n",
    "np.sqrt(mean_squared_error(y_test, rcv.predict(X_test_processed)))</code>\n",
    "    </details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2285c3f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
