{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Objectives\" data-toc-modified-id=\"Objectives-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Objectives</a></span></li><li><span><a href=\"#Set-Up-Spark-Context\" data-toc-modified-id=\"Set-Up-Spark-Context-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Set Up Spark Context</a></span></li><li><span><a href=\"#Loading-and-Preprocessing-the-Example-Data\" data-toc-modified-id=\"Loading-and-Preprocessing-the-Example-Data-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Loading and Preprocessing the Example Data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Process-the-Features\" data-toc-modified-id=\"Process-the-Features-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Process the Features</a></span></li></ul></li><li><span><a href=\"#Train-and-Predict-with-Random-Forest\" data-toc-modified-id=\"Train-and-Predict-with-Random-Forest-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Train and Predict with Random Forest</a></span></li><li><span><a href=\"#Evaluate-the-Model\" data-toc-modified-id=\"Evaluate-the-Model-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Evaluate the Model</a></span></li><li><span><a href=\"#Using-Pipeline-and-Performing-a-Grid-Search-for-Optimal-Parameters\" data-toc-modified-id=\"Using-Pipeline-and-Performing-a-Grid-Search-for-Optimal-Parameters-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Using Pipeline and Performing a Grid Search for Optimal Parameters</a></span><ul class=\"toc-item\"><li><span><a href=\"#Evaluate-with-Cross-Validation-to-Find-Optimal-Model\" data-toc-modified-id=\"Evaluate-with-Cross-Validation-to-Find-Optimal-Model-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Evaluate with Cross Validation to Find Optimal Model</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/flatiron-school/ds-spark/blob/main/spark-ml.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5c378d7f-7259-4e33-bbaf-7627af7cbb7a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark==3.2.1 in /Users/josephmata/opt/anaconda3/lib/python3.9/site-packages (3.2.1)\n",
      "Requirement already satisfied: py4j==0.10.9.3 in /Users/josephmata/opt/anaconda3/lib/python3.9/site-packages (from pyspark==3.2.1) (0.10.9.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "The operation couldn’t be completed. Unable to locate a Java Runtime.\n",
      "Please visit http://www.java.com for information on installing Java.\n",
      "\n",
      "Requirement already satisfied: mlflow in /Users/josephmata/opt/anaconda3/lib/python3.9/site-packages (1.27.0)\n",
      "Requirement already satisfied: click>=7.0 in /Users/josephmata/opt/anaconda3/lib/python3.9/site-packages (from mlflow) (8.0.3)\n",
      "Requirement already satisfied: gitpython>=2.1.0 in /Users/josephmata/opt/anaconda3/lib/python3.9/site-packages (from mlflow) (3.1.27)\n",
      "Requirement already satisfied: numpy in /Users/josephmata/opt/anaconda3/lib/python3.9/site-packages (from mlflow) (1.22.3)\n",
      "Requirement already satisfied: pytz in /Users/josephmata/opt/anaconda3/lib/python3.9/site-packages (from mlflow) (2021.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/josephmata/opt/anaconda3/lib/python3.9/site-packages (from mlflow) (6.0)\n",
      "Requirement already satisfied: databricks-cli>=0.8.7 in /Users/josephmata/opt/anaconda3/lib/python3.9/site-packages (from mlflow) (0.17.0)\n",
      "Requirement already satisfied: docker>=4.0.0 in /Users/josephmata/opt/anaconda3/lib/python3.9/site-packages (from mlflow) (5.0.3)\n",
      "Requirement already satisfied: scipy in /Users/josephmata/opt/anaconda3/lib/python3.9/site-packages (from mlflow) (1.7.1)\n",
      "Requirement already satisfied: prometheus-flask-exporter in /Users/josephmata/opt/anaconda3/lib/python3.9/site-packages (from mlflow) (0.20.2)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in /Users/josephmata/opt/anaconda3/lib/python3.9/site-packages (from mlflow) (3.20.1)\n",
      "Requirement already satisfied: importlib-metadata!=4.7.0,>=3.7.0 in /Users/josephmata/opt/anaconda3/lib/python3.9/site-packages (from mlflow) (4.8.1)\n",
      "Requirement already satisfied: cloudpickle in /Users/josephmata/opt/anaconda3/lib/python3.9/site-packages (from mlflow) (2.0.0)\n",
      "Requirement already satisfied: alembic in /Users/josephmata/opt/anaconda3/lib/python3.9/site-packages (from mlflow) (1.8.0)\n",
      "Requirement already satisfied: gunicorn in /Users/josephmata/opt/anaconda3/lib/python3.9/site-packages (from mlflow) (20.1.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.0 in /Users/josephmata/opt/anaconda3/lib/python3.9/site-packages (from mlflow) (1.4.22)\n",
      "Requirement already satisfied: sqlparse>=0.3.1 in /Users/josephmata/opt/anaconda3/lib/python3.9/site-packages (from mlflow) (0.4.2)\n",
      "Requirement already satisfied: packaging in /Users/josephmata/opt/anaconda3/lib/python3.9/site-packages (from mlflow) (21.0)\n",
      "Requirement already satisfied: pandas in /Users/josephmata/opt/anaconda3/lib/python3.9/site-packages (from mlflow) (1.3.4)\n",
      "Requirement already satisfied: Flask in /Users/josephmata/opt/anaconda3/lib/python3.9/site-packages (from mlflow) (1.1.2)\n",
      "Requirement already satisfied: entrypoints in /Users/josephmata/opt/anaconda3/lib/python3.9/site-packages (from mlflow) (0.3)\n",
      "Requirement already satisfied: querystring-parser in /Users/josephmata/opt/anaconda3/lib/python3.9/site-packages (from mlflow) (1.2.4)\n",
      "Requirement already satisfied: requests>=2.17.3 in /Users/josephmata/opt/anaconda3/lib/python3.9/site-packages (from mlflow) (2.26.0)\n",
      "Requirement already satisfied: tabulate>=0.7.7 in /Users/josephmata/opt/anaconda3/lib/python3.9/site-packages (from databricks-cli>=0.8.7->mlflow) (0.8.10)\n",
      "Requirement already satisfied: pyjwt>=1.7.0 in /Users/josephmata/opt/anaconda3/lib/python3.9/site-packages (from databricks-cli>=0.8.7->mlflow) (2.1.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /Users/josephmata/opt/anaconda3/lib/python3.9/site-packages (from databricks-cli>=0.8.7->mlflow) (1.16.0)\n",
      "Requirement already satisfied: oauthlib>=3.1.0 in /Users/josephmata/opt/anaconda3/lib/python3.9/site-packages (from databricks-cli>=0.8.7->mlflow) (3.2.0)\n",
      "Requirement already satisfied: websocket-client>=0.32.0 in /Users/josephmata/opt/anaconda3/lib/python3.9/site-packages (from docker>=4.0.0->mlflow) (1.3.3)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /Users/josephmata/opt/anaconda3/lib/python3.9/site-packages (from gitpython>=2.1.0->mlflow) (4.0.9)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/josephmata/opt/anaconda3/lib/python3.9/site-packages (from importlib-metadata!=4.7.0,>=3.7.0->mlflow) (3.6.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/josephmata/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.17.3->mlflow) (3.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/josephmata/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.17.3->mlflow) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/josephmata/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.17.3->mlflow) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/josephmata/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.17.3->mlflow) (2022.6.15)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /Users/josephmata/opt/anaconda3/lib/python3.9/site-packages (from sqlalchemy>=1.4.0->mlflow) (1.1.1)\n",
      "Requirement already satisfied: Mako in /Users/josephmata/opt/anaconda3/lib/python3.9/site-packages (from alembic->mlflow) (1.2.1)\n",
      "Requirement already satisfied: Jinja2>=2.10.1 in /Users/josephmata/opt/anaconda3/lib/python3.9/site-packages (from Flask->mlflow) (2.11.3)\n",
      "Requirement already satisfied: itsdangerous>=0.24 in /Users/josephmata/opt/anaconda3/lib/python3.9/site-packages (from Flask->mlflow) (2.0.1)\n",
      "Requirement already satisfied: Werkzeug>=0.15 in /Users/josephmata/opt/anaconda3/lib/python3.9/site-packages (from Flask->mlflow) (2.0.2)\n",
      "Requirement already satisfied: setuptools>=3.0 in /Users/josephmata/opt/anaconda3/lib/python3.9/site-packages (from gunicorn->mlflow) (58.0.4)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/josephmata/opt/anaconda3/lib/python3.9/site-packages (from packaging->mlflow) (3.0.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/josephmata/opt/anaconda3/lib/python3.9/site-packages (from pandas->mlflow) (2.8.2)\n",
      "Requirement already satisfied: prometheus-client in /Users/josephmata/opt/anaconda3/lib/python3.9/site-packages (from prometheus-flask-exporter->mlflow) (0.11.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /Users/josephmata/opt/anaconda3/lib/python3.9/site-packages (from gitdb<5,>=4.0.1->gitpython>=2.1.0->mlflow) (5.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/josephmata/opt/anaconda3/lib/python3.9/site-packages (from Jinja2>=2.10.1->Flask->mlflow) (1.1.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Run for Google Colab environment\n",
    "!pip install pyspark==3.2.1\n",
    "!apt install openjdk-8-jdk-headless -qq\n",
    "!pip install mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c7b5bd71-ac04-4b84-9a90-2e2a3b9d3670",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml import feature\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, OneHotEncoder\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit, CrossValidator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: wget\r\n"
     ]
    }
   ],
   "source": [
    "# Get data directly from repo\n",
    "!wget https://github.com/flatiron-school/ds-spark/releases/download/v1.0/US_births_2000-2014_SSA.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Use `pyspark` to build machine learning models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Set Up Spark Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c7b5bd71-ac04-4b84-9a90-2e2a3b9d3670",
     "showTitle": false,
     "title": ""
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The operation couldn’t be completed. Unable to locate a Java Runtime.\n",
      "Please visit http://www.java.com for information on installing Java.\n",
      "\n",
      "/Users/josephmata/opt/anaconda3/lib/python3.9/site-packages/pyspark/bin/spark-class: line 96: CMD: bad array subscript\n",
      "head: illegal line count -- -1\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Java gateway process exited before sending its port number",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/zh/d6prt00115d70pxr6ctyc_180000gp/T/ipykernel_1299/1958075030.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    226\u001b[0m                             \u001b[0msparkConf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m                         \u001b[0;31m# This SparkContext may be an existing one.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m                         \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparkConf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m                     \u001b[0;31m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m                     \u001b[0;31m# by all sessions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    390\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    142\u001b[0m                 \" is not allowed as it is a security risk.\")\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgateway\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlaunch_gateway\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjvm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/java_gateway.py\u001b[0m in \u001b[0;36mlaunch_gateway\u001b[0;34m(conf, popen_kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn_info_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Java gateway process exited before sending its port number\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn_info_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Java gateway process exited before sending its port number"
     ]
    }
   ],
   "source": [
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Loading and Preprocessing the Example Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "af6b675c-1cb3-4b11-9882-1d7ba0b48d47",
     "showTitle": false,
     "title": ""
    },
    "hidden": true
   },
   "source": [
    "This example assumes that we have a holdout validation dataset somewhere else, so we don't need to perform a train-test split, we only need to perform cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a7ff68e5-303d-4c4d-92c2-7a011fb94797",
     "showTitle": false,
     "title": ""
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the file since we downloaded it earlie\n",
    "df = spark.read.format('csv').option('header', 'true').\\\n",
    "load('US_births_2000-2014_SSA.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f6263aeb-3b11-42af-a198-a568bcee1aeb",
     "showTitle": false,
     "title": ""
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.toPandas().head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Process the Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "359de235-b355-4864-a9db-e11099b59f69",
     "showTitle": false,
     "title": ""
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d39ba897-3457-4e14-b998-f25b26c409da",
     "showTitle": false,
     "title": ""
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = df.withColumn('births', df['births'].cast('int'))\n",
    "df = df.withColumn('day_of_week', df['day_of_week'].cast('int'))\n",
    "df = df.withColumn('date_of_month', df['date_of_month'].cast('int'))\n",
    "df = df.withColumn('month', df['month'].cast('int'))\n",
    "df = df.withColumn('year', df['year'].cast('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "86a93804-96e8-4885-bb5b-c54e419a0916",
     "showTitle": false,
     "title": ""
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ohe = feature.OneHotEncoder(inputCols=['date_of_month',\n",
    "                                                'day_of_week'],\n",
    "                                     outputCols=['date_vec',\n",
    "                                                  'day_vec'],\n",
    "                                     dropLast=True)\n",
    "one_hot_encoded = ohe.fit(df).transform(df)\n",
    "one_hot_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "62567225-2c4a-4ff3-8b70-a20c0a2b46af",
     "showTitle": false,
     "title": ""
    },
    "hidden": true
   },
   "source": [
    "Note the 'SparseVector' we've created!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9df01298-60df-45b1-b558-829a14477cbd",
     "showTitle": false,
     "title": ""
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "features = ['year', 'month', 'date_of_month', 'day_of_week']\n",
    "\n",
    "target = 'births'\n",
    "\n",
    "vector = VectorAssembler(inputCols=features, outputCol='features')\n",
    "vectorized_df = vector.transform(one_hot_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7565c684-3a6c-4e7f-9ac1-0c8544ce6fae",
     "showTitle": false,
     "title": ""
    },
    "hidden": true
   },
   "source": [
    "The Vector Assembler is often what we want when we're building a model in Spark. [How does the VectorAssembler work?](https://spark.apache.org/docs/2.1.0/ml-features.html#vectorassembler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b32355db-6351-431b-9bf4-7ceefef823d4",
     "showTitle": false,
     "title": ""
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "vectorized_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Train and Predict with Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5a4278a2-0a2b-4fb2-9fcc-86545c194c1a",
     "showTitle": false,
     "title": ""
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rf_model = RandomForestRegressor(featuresCol='features',\n",
    "                                 labelCol='births',\n",
    "                                 predictionCol=\"prediction\").fit(vectorized_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e2a65c25-7be6-43b3-ad0a-a0efec038fac",
     "showTitle": false,
     "title": ""
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "predictions = rf_model.transform(vectorized_df).select(\"births\", \"prediction\")\n",
    "predictions.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Evaluate the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8f61139f-c7eb-4319-9dea-d62cfce1cb6d",
     "showTitle": false,
     "title": ""
    },
    "hidden": true
   },
   "source": [
    "Let's evaluate our model! [Here](https://spark.apache.org/docs/2.2.0/mllib-evaluation-metrics.html) is a reference for the many metrics available in Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "30d59b5c-5586-4984-a964-4ff9945aeaf8",
     "showTitle": false,
     "title": ""
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "30d59b5c-5586-4984-a964-4ff9945aeaf8",
     "showTitle": false,
     "title": ""
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "evaluator = RegressionEvaluator(predictionCol='prediction', labelCol='births')\n",
    "\n",
    "evaluator.evaluate(predictions, {evaluator.metricName:\"r2\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "eb8adf31-10e4-4b94-90bc-b1d9a77407c8",
     "showTitle": false,
     "title": ""
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "evaluator.evaluate(predictions, {evaluator.metricName:\"mae\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Using Pipeline and Performing a Grid Search for Optimal Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "71e41dd8-9242-45ee-924c-8a34a8efb364",
     "showTitle": false,
     "title": ""
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "one_hot_encoder = OneHotEncoder(inputCols=['date_of_month',\n",
    "                                                'day_of_week'],\n",
    "                                     outputCols=['date_vec',\n",
    "                                                  'day_vec'],\n",
    "                                     dropLast=True)\n",
    "vector_assember = VectorAssembler(inputCols=features,\n",
    "                                  outputCol='features')\n",
    "random_forest = RandomForestRegressor(featuresCol='features',\n",
    "                                      labelCol='births')\n",
    "stages = [one_hot_encoder, vector_assember, random_forest]\n",
    "\n",
    "pipeline = Pipeline(stages=stages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7f22a487-ae2a-4c7e-85bc-a15edfdc601d",
     "showTitle": false,
     "title": ""
    },
    "hidden": true
   },
   "source": [
    "Note: The stages in a pipeline can be either *Transformers* or *Estimators*. An estimator fits a DataFrame to produce a Transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d3d1c4a4-9534-4292-b0c2-e894568b18fd",
     "showTitle": false,
     "title": ""
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "random_forest.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "37cb006e-18b0-482a-86e5-e791db4af5ba",
     "showTitle": false,
     "title": ""
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "params = ParamGridBuilder().addGrid(random_forest.numTrees, [20, 50, 100]).build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e5a25b89-173d-4125-aee4-8a669ea2e865",
     "showTitle": false,
     "title": ""
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "reg_evaluator = RegressionEvaluator(predictionCol='prediction', labelCol='births',\n",
    "                                    metricName='mae')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Evaluate with Cross Validation to Find Optimal Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "679011e7-8b9b-4f45-a39a-2b82f8424d6b",
     "showTitle": false,
     "title": ""
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cv = CrossValidator(\n",
    "    estimator=pipeline,\n",
    "    estimatorParamMaps=params,\n",
    "    evaluator=reg_evaluator,\n",
    "    parallelism=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "cf5035ad-0e62-46f4-b4b9-76bb218eff78",
     "showTitle": false,
     "title": ""
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cross_validated_model = cv.fit(df.cache())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c22a7ec3-61ff-452b-9a12-b0ecce619762",
     "showTitle": false,
     "title": ""
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cross_validated_model.avgMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "40d18fda-9c91-41a5-bd2d-853e2032d9dd",
     "showTitle": false,
     "title": ""
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cross_validated_model.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8c616ba9-a273-42ab-abd0-edd80ea2fbbb",
     "showTitle": false,
     "title": ""
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cross_validated_model.bestModel.stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1517c851-5965-4f26-a229-d0e7df3dc894",
     "showTitle": false,
     "title": ""
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cross_validated_model.bestModel.stages[2].getNumTrees"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookName": "03-spark-ml",
   "notebookOrigID": 835564910129657,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "TOC",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
