{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div style=\"color:white;\n",
    "           display:fill;\n",
    "           border-radius:5px;\n",
    "           background-color:#5642C5;\n",
    "           font-size:200%;\n",
    "           font-family:Arial;letter-spacing:0.5px\">\n",
    "\n",
    "<p width = 20%, style=\"padding: 10px;\n",
    "              color:white;\">\n",
    "Agglomerative Hierarchical Clustering\n",
    "              \n",
    "</p>\n",
    "</div>\n",
    "\n",
    "Data Science Cohort Live NYC July 2022\n",
    "<p>Phase 4: Topic 35</p>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<div align = \"right\">\n",
    "<img src=\"Images/flatiron-school-logo.png\" align = \"right\" width=\"200\"/>\n",
    "</div>\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import sys, os\n",
    "import seaborn as sns\n",
    "ex_path = os.path.abspath(os.pardir)\n",
    "if ex_path not in sys.path:\n",
    "    sys.path.append(ex_path)\n",
    "\n",
    "from src.hier_example import *\n",
    "from sklearn.datasets import make_blobs\n",
    "from src.av_link_agglom_clust import centrAggClust as ac\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, cophenet\n",
    "from scipy.spatial.distance import pdist\n",
    "from sklearn.datasets import make_blobs, make_moons, load_iris\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Often grouping within data is **hierarchical**:\n",
    "- organisms: family/genus/species/subspecies/variants\n",
    "\n",
    "- cosmic structure: superclusters, local superclusters, clusters/groups, galaxies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src = \"Images/superclster.png\" width = 800/></center>\n",
    "<center> Laniakea galactic supercluster </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In many cases: \n",
    "- only have data\n",
    "- want to discover multilevel cluster hierarchy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "KMeans was not designed for this task:\n",
    "- \"Flat\" clustering vs discovering group hierarchies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Agglomerative Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Idea is:\n",
    "- Group nearest neighbor members\n",
    "- Based on some similarity/distance scheme.\n",
    "- Then repeat.\n",
    "- Finds higher order grouping at each stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src = \"Images/hierarch.gif\" /></center>\n",
    "<center> Agglomerates at each stage. </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Clearly a bottom-up approach:\n",
    "- Use data, metric/agglomeration scheme to discover hierarchies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Decision making on cluster merging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Distance notions:\n",
    "- Euclidean or Manhattan distance\n",
    "- Hamming distance\n",
    "- Damerau-Levenshtein distance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "String comparison: Damerau-Levenshtein distance\n",
    "- Number of operations: insertions, deletions, substitutions \n",
    "- Transposition of two adjacent characters.\n",
    "\n",
    "<img src = \"Images/damerau.gif\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Merge criterion: cluster distance evaluation\n",
    "\n",
    "Given a distance measure: still there are different ways to evaluate **cluster** distance.\n",
    "\n",
    "- Known as **linkage** (i.e. how do we link clusters?)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Merge clusters with minimum linkage value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src = \"Images/linkage_type.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Types of hierarchical agglomerative clustering \n",
    "\n",
    "The way that distance is measured between clusters is called the model's **linkage**.\n",
    "\n",
    "- Single Linkage \n",
    "    -  Minimum pair-wise distance: for any two clusters, take one observation from each and determine their distance. Do this over and over, until you have identified the overall minimum pair-wise distance. \n",
    "- Complete Linkage\n",
    "    -  Complete linkage may be defined as the furthest (or maximum) distance between two clusters. That is, all possible pairwise distances between elements (one from cluster A and one from B) are evaluated and the largest value is used as the distance between clusters A & B. This is sometimes called complete linkage and is also called furthest neighbor.\n",
    "- Average Linkage\n",
    "    - The distance between clusters is defined as the average distance between the data points in the clusters. \n",
    "- Ward Linkage\n",
    "    -  Ward method finds the pair of clusters that leads to minimum increase in total within-cluster variance after merging at each step.\n",
    "\n",
    "Each linkage method has it's uses:\n",
    "\n",
    "[This article](https://towardsdatascience.com/understanding-the-concept-of-hierarchical-clustering-technique-c6e8243758ec) describes the pros and cons of each approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Agglomerative Hierarchical Clustering in Python\n",
    "- See it in action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Scipy has nice functionality for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# defines metric and calculates pair-wise distance between points \n",
    "from scipy.spatial.distance import pdist \n",
    "\n",
    "#for reshaping distance matrix \n",
    "from scipy.spatial.distance import squareform\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Load in some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "agg_data = pd.read_csv('Data/Aggregation.txt', header = None, usecols = [0,1], delimiter = '\\t')\n",
    "agg_data.columns = ['X', 'Y']\n",
    "agg_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "agg_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "agg_data.plot(x = 'X', y = 'Y', kind = 'scatter');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Some hierarchical grouping here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "At each step:\n",
    "\n",
    "- algorithm calculates matrix of linkage distances between clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#computes the pairwise distance and returns a condensed array\n",
    "#removes diagonal terms and flattens array\n",
    "condensed_dist = pdist(agg_data, metric = 'euclidean' ) \n",
    "condensed_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Inspect in familiar matrix form\n",
    "- Each column(row) indexes cluster\n",
    "- Starting: each data points is their own cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sqp = squareform(condensed_dist)\n",
    "\n",
    "sqpdf = pd.DataFrame(sqp, index = agg_data.index,\n",
    "            columns = agg_data.index)\n",
    "\n",
    "sqpdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "At each iteration, merges clusters with minimum pairwise distance:\n",
    "- Merged cluster is new *supercluster*\n",
    "- For $n$ datapoints continues this $n-1$ times.\n",
    "- Entire sequence of merges gives us entire hierarchy of clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src = \"Images/hierarch.gif\" /></center>\n",
    "<center> Agglomerates at each stage. </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "scipy's linkage function:\n",
    "- linkage(condensed pair-wise distance, linkage method, metric)\n",
    "- returns an entire sequence of cluster merges in agglomerative process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import linkage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "np_linkage = linkage(condensed_dist, method='ward', metric='euclidean' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "cols = ['clst1', 'clst2', 'dist', 'num_points']\n",
    "linkage_df = pd.DataFrame(np_linkage,\n",
    "                          columns = cols)\n",
    "linkage_df.index.name = 'merge'\n",
    "linkage_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Rows: Cluster merges\n",
    "\n",
    "- 788 data points: 788 - 1 merges\n",
    "\n",
    "- Columns: clusters merged, distance between clusters, number of points in new cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(linkage_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Creating the dendrogram\n",
    "- scipy's dendrogram function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (5,7))\n",
    "dendrogram(linkage_df, no_labels = True, leaf_font_size = 8, color_threshold = 100,  orientation = 'left' )\n",
    "ax.set_xlabel('Linkage distance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "At given linkage distance threshold:\n",
    "- want to output \"flattened\" cluster assignments:\n",
    "- scipy's fcluster function: fcluster(linkage_matrix, threshold, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import fcluster\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# criterion as linkage distance is fairly common\n",
    "clust_assgn = fcluster(linkage_df, t = 100, criterion = 'distance')\n",
    "\n",
    "#put labels into dataframe and scatterplot\n",
    "agg_assigned = deepcopy(agg_data)\n",
    "agg_assigned['label'] = clust_assgn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "agg_assigned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Scatterplot with hue on cluster assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "n_labels = len(agg_assigned['label'].unique())\n",
    "fig, ax = plt.subplots(figsize = (6,4))\n",
    "sns.scatterplot(x = 'X', y = 'Y',\n",
    "                hue = 'label', \n",
    "                data = agg_assigned, \n",
    "                palette=sns.color_palette(\n",
    "                    'tab10', n_labels), ax = ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def plot_with_threshold(data, linkage, threshold = 100):\n",
    "    \n",
    "    clust_assgn = fcluster(linkage, t = threshold, criterion = 'distance')\n",
    "    \n",
    "    agg_assigned = deepcopy(data)\n",
    "    agg_assigned['label'] = clust_assgn\n",
    "    n_labels = len(agg_assigned['label'].unique())\n",
    "    \n",
    "\n",
    "    fig, ax = plt.subplots(1,2, figsize = (8,5))\n",
    "    d = dendrogram(linkage, no_labels = True, leaf_font_size = 8,\n",
    "               color_threshold = threshold, orientation = 'left', ax = ax[0] )\n",
    "    ax[0].set_xlabel('Linkage distance')\n",
    "    \n",
    "    ax[0].axvline(threshold, c = 'b', linestyle = '--')\n",
    "    \n",
    "    #NEW CODE\n",
    "    ax[1].scatter(agg_assigned['X'].iloc[d['leaves']], \n",
    "                  agg_assigned['Y'].iloc[d['leaves']], \n",
    "                  color=d['leaves_color_list'])\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "plot_with_threshold(agg_data, linkage_df, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Evaluating the agglomerative linkage and dendrogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Cophenetic correlation coefficient**\n",
    "\n",
    "- correlation between point pairwise distance and linkage distance at which pairs are joined.\n",
    "\n",
    "- how closely related is the agglomerative clustering to the underlying data structure?\n",
    "\n",
    "\n",
    "- Is a correlation coefficient: \n",
    "    - 1 = perfect correlation\n",
    "    - 0 = no correlation\n",
    "    - -1 = anti-correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Cophenetic distances\n",
    "\n",
    "- Taking point $i$ and $j$: \n",
    "- linkage distance between respective clusters before merge.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src = \"Images/cophen_dist.jpg\" width = 400/></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import cophenet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Takes in linkage matrix and reduced pairwise distance matrix:\n",
    "- returns correlation and list of cophenetic distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# evaluate effectiveness of hierarchical clustering with cophenetic correlation\n",
    "corr_coef, cophen_dist = cophenet(np_linkage, condensed_dist)\n",
    "print(corr_coef)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The cophenetic correlation is measuring how effective our agglomerative clustering is at capturing the entire **hierarchy**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Hierarchical clustering with `sklearn` \n",
    "\n",
    "- Scipy has much better functionality for exploring full hierarchy\n",
    "- Sklearn for using hierarchical clustering in flat clustering/prediction tasks\n",
    "- can specify number of clusters, adjusts threshold automatically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# affinity is distance metric(euclidean, manhattan)\n",
    "# linkage (average, complete, single, ward, etc.)\n",
    "agg_cluster = AgglomerativeClustering(n_clusters = 7,\n",
    "                                      affinity = 'euclidean', \n",
    "                                      linkage = 'average')\n",
    "agg_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(x = 'X', y = 'Y', data = agg_data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "agg_labels = agg_cluster.fit_predict(agg_data)\n",
    "sklearn_agg_data = deepcopy(agg_data)\n",
    "sklearn_agg_data['labels'] = agg_labels\n",
    "sklearn_agg_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(x = 'X', y = 'Y', \n",
    "                hue = 'labels', palette = 'tab10',\n",
    "                data = sklearn_agg_data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Benefit of sklearn implementation:\n",
    "- Chooses distance threshold for you given number of clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Evaluation\n",
    "\n",
    "Operating in this flat mode:\n",
    "- getting reasonable cluster number -- silhouette score.\n",
    "- not a sure fire method but not terrible either."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def plot_silh_scores(X, K, increment, linkage):\n",
    "\n",
    "    klist = np.arange(2,K,increment)\n",
    "    score_list = []\n",
    "    for k in klist:\n",
    "        agg_clstrer = AgglomerativeClustering(n_clusters = k,\n",
    "                                      affinity = 'euclidean', \n",
    "                                      linkage = linkage)\n",
    "        \n",
    "        clstr_labels = agg_clstrer.fit_predict(X)\n",
    "\n",
    "        score = silhouette_score(X, clstr_labels)\n",
    "        score_list.append(score)\n",
    "        \n",
    "    sns.lineplot(x = klist, y = score_list, color = 'r')\n",
    "    plt.ylabel('Silhouette Coefficient')\n",
    "    plt.xlabel('K')\n",
    "    plt.title('Silhouette coefficient plot')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plot_silh_scores(agg_data, 15, 1, 'average')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Under average linkage:\n",
    "- k = 6 might be a good choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def plot_with_sklearn(X, k, linkage):\n",
    "    agg_cluster = AgglomerativeClustering(n_clusters = k,\n",
    "                                      affinity = 'euclidean', \n",
    "                                      linkage = linkage)\n",
    "    \n",
    "    agg_labels = agg_cluster.fit_predict(X)\n",
    "    sklearn_agg_data = deepcopy(X)\n",
    "    sklearn_agg_data['labels'] = agg_labels\n",
    "    \n",
    "    sns.scatterplot(x = 'X', y = 'Y', \n",
    "                hue = 'labels', palette = 'tab10',\n",
    "                data = sklearn_agg_data)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Seven is actually the optimal number for the flat clustering.\n",
    "- Clustering is a tricky business and often requires human judgement.\n",
    "- Sklearn's API makes it easy for agglomerative clustering: don't have to mess with thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plot_with_sklearn(agg_data, 6, 'average')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "plot_with_sklearn(agg_data, 7, 'average')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "But:\n",
    "\n",
    "- agglomerative hierarchical clustering for flat clustering tasks: SLOW.\n",
    "- designed to calculate whole hierarchy.\n",
    "- better methods for flat clustering: kmeans, DBSCAN, mixture methods, variational autoencoders, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Advantages & Disadvantages of hierarchical clustering\n",
    "\n",
    "#### Advantages\n",
    "- Intuitive and easy to implement\n",
    "- More informative than k-means because it takes individual relationship into consideration\n",
    "- Allows us to look at dendrogram and decide number of clusters\n",
    "\n",
    "#### Disadvantages\n",
    "- Very sensitive to outliers\n",
    "- Cannot undo the previous merge, which might lead to problems later on "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Further reading\n",
    "\n",
    "- [from MIT on just hierarchical](http://web.mit.edu/6.S097/www/resources/Hierarchical.pdf)\n",
    "- [from MIT comparing clustering methods](http://www.mit.edu/~9.54/fall14/slides/Class13.pdf)\n",
    "- [fun CMU slides on clustering](http://www.cs.cmu.edu/afs/andrew/course/15/381-f08/www/lectures/clustering.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
