{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div style=\"color:white;\n",
    "           display:fill;\n",
    "           border-radius:5px;\n",
    "           background-color:#5642C5;\n",
    "           font-size:200%;\n",
    "           font-family:Arial;letter-spacing:0.5px\">\n",
    "\n",
    "<p width = 20%, style=\"padding: 10px;\n",
    "              color:white;\">\n",
    "Neural Networks: Architecture\n",
    "              \n",
    "</p>\n",
    "</div>\n",
    "\n",
    "Data Science Cohort Live NYC Feb 2022\n",
    "<p>Phase 4: Topic 40</p>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<div align = \"right\">\n",
    "<img src=\"Images/flatiron-school-logo.png\" align = \"right\" width=\"200\"/>\n",
    "</div>\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join(os.pardir, os.pardir))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### What is a neural network?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Computational graph made of layers of composite calculation units:\n",
    "    - designed to compute a function mapping inputs to outputs. \n",
    "- Each unit/layer gets tuned during training:\n",
    "    - learns some specific part of input-output mapping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center> <img src = \"images/dogcat.gif\" width = 500 > </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The many interconnections: important\n",
    "    \n",
    "- Let nodes at each layer use relations it learned in adjacent layers.\n",
    "\n",
    "- Build high degree of flexibility: **can learn very complex functions**\n",
    "\n",
    "- Use connections in model to learn what aspects of data to rely on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src = \"images/dogcat.gif\" width = 500 ></center?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Function complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src = \"Images/neural-networks-layers.webp\" width = 600 >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Can learn complex functions and decision boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's see how a neural network is learning at each layer:\n",
    "\n",
    "<a href = \"http://playground.tensorflow.org\" >Tensorflow Playground</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Neural network: minimizing objective function (squared loss/binary cross-entropy)\n",
    "- tune connections\n",
    "- each node/unit learning features in the process\n",
    "- feeds features to next layer. Learns more complex features to predict with, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Complexity sufficient to learn/generalize on some pretty difficult problems:\n",
    "\n",
    "<center><img src = \"Images/image_multiclass.png\" width = 400 ></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### But how does all this work?\n",
    "- Back to basics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<img src = \"Images/simple_nn.jfif\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Composition of a single unit** \n",
    "- can be thought of as a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src = \"Images/single-unit.png\" width = 500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- If $f$ is identity matrix: literally linear regression (depending on objective function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "$$ \\text{Output} = x_1 w_1 + x_2 w_2  + x_3 w_3 + b $$\n",
    "\n",
    "In vector form: \n",
    "\n",
    "$$ \\text{Output} = \\textbf{w}^T \\textbf{x} + \\textbf{b} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src = \"Images/single-unit.png\" width = 500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Goal: tune weights $\\textbf{w}$ to minimize objective function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Relation to logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src = \"Images/single-unit.png\" width = 500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now $f$ is the sigmoid function:\n",
    "\n",
    "$$ f = \\sigma(w_1x_1+...+w_nx_n)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src = \"Images/linear_vs_logistic_regression.png\" >\n",
    "<center> Linear doesn't model well</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### The role of f: the activation function\n",
    "\n",
    "- Linear: turns out to be too simple in general.\n",
    "- Want neuron to learn some non-trivial part of function.\n",
    "\n",
    "**Key is adding nonlinearity through $f$** (often referred to as $g$ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src = \"Images/activation_func.png\" >\n",
    "Typical choices for activation function $g$</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**ReLU: the most common activation function**\n",
    "\n",
    "- simple thresholding behavior\n",
    "- automated feature selection: \n",
    "    - turn off/on node depending on feature/data in previous steps to network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src = \"Images/relu.png\" >\n",
    "    If input < 0: turn off neuron/corresponding feature.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Putting many units together: form a layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src = \"images/activations_nn_layer.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Matrix/vector representation \n",
    "\n",
    "$$ \\textbf{a}^{[1]} = g(\\textbf{z}^{[1]})$$\n",
    "\n",
    "$$ \\textbf{z}^{[1]} = W^{[1]T} \\textbf{x} + \\textbf{b}^{[1]}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src = \"images/activations_nn_layer.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Matrix/vector representation \n",
    "\n",
    "$$ \\textbf{a}^{[1]} = g(\\textbf{z}^{[1]})$$\n",
    "\n",
    "$$ \\textbf{z}^{[1]} = W^{[1]T} \\textbf{x} + \\textbf{b}^{[1]} \\\\ = \\begin{bmatrix}\n",
    "           z^{[1]}_{1} \\\\\n",
    "           z^{[1]}_{2} \\\\\n",
    "           z^{[1]}_{3} \\\\\n",
    "           z^{[1]}_{4} \\\\\n",
    "         \\end{bmatrix} = \\left[\n",
    "  \\begin{array}{ccc}\n",
    "    \\rule[.5ex]{3.5em}{0.4pt} & \\textbf{w}^{T}_{1} & \\rule[.5ex]{3.5em}{0.4pt}\\\\\n",
    "    \\rule[.5ex]{3.5em}{0.4pt} & \\textbf{w}^{T}_{2} & \\rule[.5ex]{3.5em}{0.4pt} \\\\\n",
    "          \\rule[.5ex]{3.5em}{0.4pt}   & \\textbf{w}^{T}_{3}    &     \\rule[.5ex]{3.5em}{0.4pt}     \\\\\n",
    "    \\rule[.5ex]{3.5em}{0.4pt} & \\textbf{w}^{T}_{4} & \\rule[.5ex]{3.5em}{0.4pt}\n",
    "  \\end{array}\n",
    "\\right] \n",
    "\\begin{bmatrix}\n",
    "           x_{1} \\\\\n",
    "           x_{2} \\\\\n",
    "           x_{3} \\\\\n",
    "         \\end{bmatrix}\n",
    "+ \\textbf{b}^{[1]}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Expanded further: matrix of weights/bias vector to learn.\n",
    "\n",
    "$$ \\textbf{z}^{[1]} = W^{[1]T} \\textbf{x} + \\textbf{b}^{[1]} \\\\ = \\begin{bmatrix}\n",
    "           z^{[1]}_{1} \\\\\n",
    "           z^{[1]}_{2} \\\\\n",
    "           z^{[1]}_{3} \\\\\n",
    "           z^{[1]}_{4} \\\\\n",
    "         \\end{bmatrix} = \\left[\n",
    "  \\begin{array}{ccc}\n",
    "    w_{11} & w_{12} & w_{13}\\\\\n",
    "    w_{21} & w_{22} & w_{23} \\\\\n",
    "    w_{31} & w_{32} & w_{33} \\\\    \n",
    "    w_{41} & w_{42} & w_{43} \\\\\n",
    "  \\end{array}\n",
    "\\right] \n",
    "\\begin{bmatrix}\n",
    "           x_{1} \\\\\n",
    "           x_{2} \\\\\n",
    "           x_{3} \\\\\n",
    "         \\end{bmatrix}\n",
    "+ \\begin{bmatrix}\n",
    "           b_{1}^{[1]} \\\\\n",
    "           b_{2}^{[1]} \\\\\n",
    "           b_{3}^{[1]} \\\\\n",
    "           b_{4}^{[1]} \n",
    "           \\\\\n",
    "         \\end{bmatrix}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Deeper neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src = \"images/Deeper_network.jpg\" > Activations and weights for each layer </center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src = \"images/Deeper_network.jpg\" > Feedforward: pass data in, calculate activations. Flows from input to output through hidden layers. </center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Hidden layers: layers between input/output layer.\n",
    "- Neural network learning features here automatically.\n",
    "- Don't really know/control what hidden layers are finding: **black box** \n",
    "    - inputs/outputs in hidden layers are hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "heading_collapsed": true,
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Forward Propagation**\n",
    "\n",
    "- Pass input: compute function map layer by layer using weights\n",
    "- yield output $\\hat{y}$ (regression target, class label, etc.)\n",
    "- Compute cost function $L(\\hat{y},y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Single neuron network (regression):\n",
    "<center><img src = \"Images/costfunction_singleexample.png\" width = 700></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Implement **forward propagation** for single neuron.\n",
    "\n",
    "Dataset: handwritten numbers from `sklearn`:\n",
    "- Each record is a 64-bit (8x8) image of a handwritten number between 0 and 9. \n",
    "- Each pixel value (a number between 0 and 16) represents the relative brightness of the pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "digits = load_digits()\n",
    "flat_image = np.array(digits.data[0]).reshape(digits.data[0].shape[0], -1)\n",
    "eight_by_eight_image = digits.images[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's look at one digit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "digits = load_digits()\n",
    "eight_by_eight_image = digits.images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "imgplot = plt.imshow(eight_by_eight_image,\n",
    "                     cmap='Greys')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Image in matrix form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# larger numbers = darker\n",
    "\n",
    "eight_by_eight_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Image data fed into neuron: must be in vector form:\n",
    "- **flatten** the image into a 64x1 array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "img_flattened = np.ravel(eight_by_eight_image)\n",
    "img_flattened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(img_flattened.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Want to reshape into column vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "hidden": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "img_col_vec = img_flattened.reshape(-1,1)\n",
    "print(img_col_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(img_col_vec.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Forward propagate data through single neuron:\n",
    "<center><img src = \"Images/single-unit.png \" width = 450></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We will instantiate our weight with small random numbers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "hidden": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "w = np.random.uniform(-0.1, 0.1, (flat_image.shape[0], 1))\n",
    "print(w[:5])\n",
    "print(w.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We'll set our bias term to 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "b = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Summation\n",
    "\n",
    "Calculate the summation (linear part of calculation):\n",
    "\n",
    "<center><img src = \"Images/single-unit.png \" width = 450></center>\n",
    "\n",
    "Dot product: $\\textbf{w}^T \\textbf{x} + b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "z = w.T@img_col_vec + b\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The activation function\n",
    "\n",
    "Nonlinear part of calculation:\n",
    "\n",
    "<center><img src = \"Images/single-unit.png \" width = 450></center>\n",
    "\n",
    "We have a suite of activation functions to choose from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "heading_collapsed": true,
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Sigmoid\n",
    "\n",
    "$f(x) = \\frac{1}{1+e^{-x}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Typically used in last node of network doing binary classification:\n",
    "\n",
    "<img src = \"Images/sigmoid_binary.png\" width = 350>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Z is the input from our collector, the sum of the weights\n",
    "# multiplied by the features and the bias\n",
    "\n",
    "def sigmoid(z):\n",
    "    '''\n",
    "    Input the sum of our weights times the pixel intensities, plus the bias\n",
    "    Output a number between 0 and 1.\n",
    "    \n",
    "    '''\n",
    "    return 1/(1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X = np.linspace(-10, 10, 20000)\n",
    "sig = sigmoid(X)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(X, sig)\n",
    "ax.set_title('Sigmoid Activation');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Calculating neuron output if sigmoid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "a = sigmoid(z)\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "heading_collapsed": true,
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### tanh\n",
    "\n",
    "- shifted/scaled version of sigmoid(roughly)\n",
    "- when used, typically activation for hidden layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$f(x) = tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def tanh(z):\n",
    "    return (np.exp(z) - np.exp(-z)) / (np.exp(z) + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Coding tanh:\n",
    "\n",
    "X = np.linspace(-10, 10, 20000)\n",
    "y_tanh = tanh(X)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(X, y_tanh)\n",
    "ax.set_title('Hyperbolic Tangent Activation');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Shifted/scaled (roughly) compared  to sigmoid:\n",
    "\n",
    "- output is centered around 0\n",
    "- the output is between -1 and 1\n",
    "- steeper slope vs input (more sensitivity) \n",
    "- makes learning in the next layer easier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "One problem with tanh (and sigmoid) for hidden layers is that for large parts of input space: \n",
    "\n",
    "- the slope of the activation function flattens out\n",
    "\n",
    "Doesn't learn from features effectively in these regions\n",
    "- tuning weights based off information in these regions (ineffective and slow)\n",
    "- **vanishing gradient** problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "heading_collapsed": true,
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### ReLU\n",
    "\n",
    "- superseded tanh in hidden layers (for the most part)\n",
    "- de facto non-output activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$f(x) = 0$ if $x\\leq 0$; $f(x) = x$ otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    return (z * (z > 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Coding ReLU:\n",
    "\n",
    "X = np.linspace(-10, 10, 200)\n",
    "\n",
    "y_relu = [relu(x) for x in X]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(X, y_relu)\n",
    "ax.set_title('ReLU Activation');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Constant gradient leads to faster learning in comparison to sigmoid and tanh.\n",
    "\n",
    "- Region of 0 activation: easy computation.\n",
    "\n",
    "Models notion of neuron on/off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "heading_collapsed": true,
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Softmax\n",
    "\n",
    "$$ \\large \\sigma(\\textbf{z})_i = \\frac{e^{z_i}}{\\sum_{j}e^{z_j}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src = \"Images/softmax.png\" width = 400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Appropriate activation in the output layer for **multi-class** classification problems. \n",
    "\n",
    "- Outputting the probabilities of belonging to each class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "There are other activation functions; [see here](https://towardsdatascience.com/comparison-of-activation-functions-for-deep-neural-networks-706ac4284c8a). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Our nodes will be taking in input from multiple sources. Let's add the entire training set as our input. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(digits.data,\n",
    "                                                    digits.target,\n",
    "                                                    random_state=42,\n",
    "                                                    test_size=0.2)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X_train[0, :].reshape(8, 8) # first example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "imgplot = plt.imshow(X_train[0, :].reshape(8, 8),\n",
    "                     cmap='Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Feeding all training data into single neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "z_0 = X_train@(w)+b\n",
    "z_0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "z_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "hidden": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# calculating sigmoid activation for each data point\n",
    "a_0 = sigmoid(z_0)\n",
    "a_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "calculating ReLU for each training data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "a_0_relu = np.array([relu(z) for z in z_0])\n",
    "a_0_relu[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "a_0_relu.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now we compute with matrix of weights:\n",
    "\n",
    "layer with 4 neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "w_1 = np.random.normal(0, 0.1, (X_train.shape[1], 4))\n",
    "w_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "b_1 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "Z_1 = X_train@w_1 + b_1\n",
    "Z_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "A_1 = relu(Z_1)\n",
    "A_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now each of these neurons has a set of weights and a bias associated with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "w_2 = np.random.normal(0, 0.1, (A_1.shape[1], 1))\n",
    "\n",
    "w_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "b_2 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "Z_2 = A_1.dot(w_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "output = sigmoid(Z_2)\n",
    "y_pred = output > 0.5\n",
    "y_hat = y_pred.astype(int)\n",
    "y_hat[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The **backpropagation** algorithm: adjusting the parameters (weights) to get a better result. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Propagating the error (averaged over all training samples) back through the network:\n",
    "\n",
    "- with the many-sample cost function $$J(\\{W^{[l]}\\}, \\{b^{[l]}\\}) = \\frac{1}{m} \\sum_{i = 1}^m L(\\hat{y_i}, y_i)$$ guiding us.\n",
    "\n",
    "\n",
    "\n",
    "- Goal: Tune $\\{W^{[l]}\\}$ and $ \\{b^{[l]}\\}$ to minimize $J$.\n",
    "\n",
    "**How?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We do this tuning by propagating the (average) error back through the network, with the cost function $J$ guiding us and adjusting via gradient descent.\n",
    "\n",
    "> Turn down previous neurons that give a bad result\n",
    ">\n",
    "> Turn up previous neurons that give a good result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Gradient descent algorithm**\n",
    "- Compute $\\frac{\\partial J}{\\partial W^{[l]}}$ and $\\frac{\\partial J}{\\partial b^{[l]}}$ for each layer $l$.\n",
    "- Update each bias vector/weight matrix:\n",
    "$$  W^{[l]} \\rightarrow W^{[l]} - \\alpha \\frac{\\partial J}{\\partial W^{[l]}}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "***Backpropagation allows very efficient computation of gradients at each layer***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Then forward prop again. Repeat back prop...cycle until $J$ converges:\n",
    "\n",
    "<img src = \"Images/backprop.gif\" width = 500>\n",
    "\n",
    "**Update made to weights after computing all gradients traversing in backpropagation.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Great video explanation of backpropagation by 3Blue1Brown (part of a full playlist): [Backpropagation calculus | Deep learning, chapter 4](https://www.youtube.com/watch?v=tIeHLnjs5U8&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&index=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We'll talk more about this in optimizing our neural networks but some hyperparameters include:\n",
    "\n",
    "- **Learning Rate ($\\alpha$)**: how big of a step we take in gradient descent\n",
    "- **Number of Epochs**: how many times we repeat this process\n",
    "- **Batch Size**: how many data points we use in a single training session (1 epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
